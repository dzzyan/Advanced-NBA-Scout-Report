---
title: "131 Final Project"
author: "Seth Marceno(8934838), Derek Yan(4377347)"
date: "12/10/2019"
output: pdf_document
---


```{r, include=FALSE}
library(kableExtra)
library(readr)
library(dplyr)
library(ggplot2)
library(ggridges)
library(maps)
library(tidyverse)
library(tidyr)
library(tree)
library(maptree)
library(glmnet)
library(ROCR)
library(MASS)

load('FinalProject.RData')

election.raw <- read_delim("data/election/election.csv", delim = ",") %>% mutate(candidate=as.factor(candidate))

census_meta <- read_delim("data/census/metadata.csv", delim = ";", col_names = FALSE) 
census <- read_delim("data/census/census.csv", delim = ",") 
```


# 1. 
Voter behavior prediction is a hard problem becuase modeling human behavior is nearly an impossible task. The models that we use to predict elections are not always accurate. For many instances there are predictors that can be measured such as the economy or changing opinions due to powerful political advertisements. Similarly, predictors that are measureable are subject to survivorship bias. For instance, online polls for candidates are voluntary or can be found on certain websites, or some people may lie about who they vote for; thus, these results do not encompass the populations feelings and the corrections statisticians try to make may not be accurate.  


# 2. 
Nate Silver's approach was unique because he had to add time series to his model and when accounting for the random variation of pollings from each state, Silver didnt look at the maximum variation, he took into a ccount the full range of probabilities. Polls are prior to actual voting and therefore dont accurately model the populations feelings at the time of the actual election. This can be accounted for by generating a time series that helps to model changing intentions, by using the full range of probabilites instead of the maximum variation, he is able to account better for the change in support for a given candidate.


# 3. 
In 2016 aggregated polls missed the correct results in many important swing states. This mistake led to a miscalculation of final results for the election. However, this miscalculation for these individual swing states tended to overstate the margin in which Clinton was ahead; furthermore, the national polls were off in the same direction. The bigger the lead Trump was predicted in a given state, the more he outperformed his polls. In order to make predictions better in the future, we must come up with ways to model a degree of uncertainty. For instance, voter turnout was lower than expected, as well as the amount of people who were unwilling to vocally admit who they supported. Therefore if models in the future can account for these degrees of uncertainty, we will see more accurate results. 



```{r, echo=FALSE}
kable(election.raw %>% filter(county == "Los Angeles County"))  %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width=FALSE)
```




#4. 
Looking at election.raw dataset, we find that it is a data frame with 18345 rows and 5 columns of variables.
```{r, echo=FALSE}
election.raw <- subset(election.raw, fips!=2000)
first <- c('The # of rows in election.raw is', dim(election.raw)[1])
second <- c('The # of columns in election.raw is', dim(election.raw)[2])
```


Because Alaska is a very small state (in terms of population), we remove those from the election data of fips corresponding to this specific voting county in Alaska because this data is already presented for the state of Alaska. Therefore by removing fips = 2000 in this data set, we are removing redundant data.




# 5. 

```{r, echo=FALSE}
election_federal <- subset(election.raw, fips == 'US')
election_state <- subset(election.raw, is.na(county) == TRUE & fips != 'US')
election <- subset(election.raw, is.na(county) == FALSE)
```

Going forward we will have three subsets of the data election.raw, federal results for all of the candidates, state results for each candidate, and county results for each candidate. 

# 6. 

In the 2016 presidential race, there were 32 named presidential candidates. 

```{r, echo=FALSE, warning=FALSE}

subset_1 <- subset(election_federal, election_federal$candidate==unique(election_federal$candidate)[1:8])

subset_2 <- subset(election_federal, election_federal$candidate==unique(election_federal$candidate)[9:16])

subset_3 <- subset(election_federal, election_federal$candidate==unique(election_federal$candidate)[17:24])

subset_4 <- subset(election_federal, election_federal$candidate==unique(election_federal$candidate)[25:32])





ggplot(data=subset_1, aes(x=subset_1$candidate, y=subset_1$votes )) +
  geom_bar(stat="identity", width=0.5, size = 0.3) +
  xlab('Candidate') + 
  ylab('Votes') +
  theme(axis.text=element_text(size=7))

ggplot(data=subset_2, aes(x=subset_2$candidate, y=subset_2$votes )) +
  geom_bar(stat="identity", width=0.5, size = 0.3)+
  xlab('Candidate') + 
  ylab('Votes') +
  theme(axis.text=element_text(size=7))

ggplot(data=subset_3, aes(x=subset_3$candidate, y=subset_3$votes )) +
  geom_bar(stat="identity", width=0.5, size = 0.3)+
  xlab('Candidate') + 
  ylab('Votes') +
  theme(axis.text=element_text(size=7))


ggplot(data=subset_4, aes(x=subset_4$candidate, y=subset_4$votes )) +
  geom_bar(stat="identity", width=0.5, size = 0.3)+
  xlab('Candidate') + 
  ylab('Votes') +
  theme(axis.text=element_text(size=7))

```



# 7.

Now we are going to creat two new variables county_winner and state_winner which are data frames containing the winner in each county for county_winner, and the winner in each state for state_winner.
```{r, echo=FALSE}
county_winner <- election %>% group_by(fips) %>%
  mutate(total = sum(votes), pct = votes/total) %>%
  top_n(n = 1, wt = pct)


state_winner <- election_state %>% group_by(state) %>%
  top_n(n=1, wt = votes)

county_winner_table <- kable(head(county_winner)) %>% kable_styling(bootstrap_options = c("striped", "hover",
                                                                                               "condensed",
                                                                          "responsive"), full_width=FALSE, 
                                                              position = 'center') 

state_winner_table <- kable(head(state_winner)) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed",
                                                                                  "responsive"), full_width=FALSE, 
                                                            position = 'center') 
```



Here we have the first 6 rows of county_winner:
```{r, echo=FALSE}
county_winner_table
```



Here we see the first 6 rows of state_winner:
```{r, echo=FALSE}
state_winner_table
```



# 8. 
Here we will create a map of the United States in order to help our visualizations.


```{r, echo=FALSE}
states <- map_data("state")

ggplot(data = states) + 
  geom_polygon(aes(x = long, y = lat, fill = region, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE) 
```




Below we see the United States divided and colored by counties:

```{r, echo=FALSE}
counties <- map_data('county')

ggplot(data = counties) + 
  geom_polygon(aes(x = long, y = lat, fill = subregion, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE) 
```


# 9. 

We see on the map below each state that is blue has Clinton winning and every state that is red has Trump. 
```{r, echo=FALSE, eval=FALSE}
states <- states %>%
  mutate(fips = state.abb[match(states$region, tolower(state.name))])


left.join.states <- left_join(states, state_winner)
```


```{r, echo=FALSE}
ggplot(data = left.join.states) + 
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE) 
```


# 10.

Here we have the same map separated into regions:

```{r, echo=FALSE, eval=FALSE}
maps::county.fips
county.fips <- separate(data = county.fips, col = polyname, sep = ',', into = c('region', 'subregion'))
```

```{r, echo=FALSE, eval=FALSE}
ljc <- left_join(county.fips, county)
ljc$fips <- as.character(ljc$fips)

left.join.county <- left_join(county_winner, ljc)
```

```{r, echo=FALSE, warning=FALSE}
ggplot(data = left.join.county) + 
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE) 
```



# 11. Need to redo graph and analize

The following graphic is made from the census data package:


```{r, echo=FALSE, warning=FALSE}

ggplot(data = census, aes(x=census$IncomePerCap, y=census$State)) +
  geom_point(cex = 0.01) + 
  geom_segment( aes(x=census$IncomePerCap, xend=census$IncomePerCap, yend=census$State)) +
  xlab("Income Per Capita, By region") +
  ylab('State') 
```



# 12. 

Since the census data set contains a lot of information, we will be aggregating much of it for simplicity of use. We will be cleaning to filter out any rows with missing data and combining/deleting rows. We will call this new set census.del. 

```{r, echo=FALSE, warning=FALSE}
census.del <- census[complete.cases(census[,1:36]),]

census.del$Men <- (census.del$Men/census.del$TotalPop) * 100
#census.del$Women <- (census.del$Women/census.del$TotalPop) * 100
census.del$Citizen <- (census.del$Citizen/census.del$TotalPop) * 100
census.del$Employed <- (census.del$Employed/census.del$TotalPop) * 100

census.del <- census.del %>% mutate(minority = census.del$Black + census.del$Hispanic + 
                                      census.del$Native + census.del$Asian + census.del$Pacific)
census.del <- subset(census.del, select = -c(Hispanic, Black, Asian, Native, Pacific, 
                                             Walk, PublicWork, Construction, Women))
```


```{r, echo=FALSE, warning=FALSE}
census.subct <- group_by(census.del, State, County) %>% add_tally(TotalPop)
census.subct["wt"] <- census.subct$TotalPop/census.subct$n
```


```{r, echo=FALSE, warning=FALSE}
census.ct <- census.subct %>% summarize_at(vars(Men:minority), funs(weighted.mean(., wt)))
# census.ct1 <- census.subct %>% summarize_at(vars(Men:Citizen) , funs(weighted.mean(.,wt)))
# 
# census.ct2 <- census.subct %>% summarize_at(vars(TotalPop) , funs(sum))
# 
# census.ct3 <- census.subct %>% summarize_at(vars(Income:IncomeErr), funs(mean))
# 
# census.ct4 <- census.subct %>% summarise_at(vars(IncomePerCap), funs(weighted.mean(.,wt)))
# 
# census.ct5 <- census.subct %>% summarise_at(vars(IncomePerCapErr), funs(mean))
# 
# census.ct6 <- census.subct %>% summarise_at(vars(Poverty:minority), funs(weighted.mean(.,wt)))
# 
# census.ct <- left_join(census.ct2, census.ct1)
# census.ct <- left_join(census.ct, census.ct3)
# census.ct <- left_join(census.ct, census.ct4)
# census.ct <- left_join(census.ct, census.ct5)
# census.ct <- left_join(census.ct, census.ct6)

kable(head(census.ct[1:7])) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width=FALSE)
```


# 13. changed 29 to 28


```{r, echo=FALSE}
ct.pc <- prcomp(census.ct[3:27], center = TRUE, scale = TRUE)
subct.pc <- prcomp(census.subct[3:30], center = TRUE, scale = TRUE)

ct.pc <- as.data.frame(ct.pc$rotation)
subct.pc <- as.data.frame(subct.pc$rotation)

ct.pc <- ct.pc[1:2]
subct.pc <- subct.pc[1:2]
```


Here we have a glimpse at ct.pc:
```{r, echo=FALSE}
kable(head(ct.pc)) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                                              full_width=FALSE)
```


Here we have a glimpse at subct.pc:
```{r, echo=FALSE}
kable(head(subct.pc)) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                                              full_width=FALSE)
```


When doing principal component analysis on our county census data (ct.pc) and sub county census data(subct.pc), we chose to center because the mean of each column in our data is not zero, so we dont want the first principal component to point towards the mean. We chose to scale because we want our predictors to have variance one. Doing some exploratory analysis we find that the features: IncomePerCap, ChildPoverty, and Poverty, have the highest absolute value for the first principal component for ct.pc. For subct.pc we find the highest absolute values of the first principal component correspond to IncomePerCap, Professional, and Poverty. On top of this, for both ct.pc and sbct.pc, we see the features such as men and women have the same value for PC1 and PC2, but have opposite signs. This is because these values are directly correlated with each other, specifically, the perventage of women in a given county is just 1 - the percentage of men. 


# 14. changed 29 to 28

```{r, echo=FALSE}
ct.pca <- prcomp( census.ct[3:27], scale = TRUE )
subct.pca <- prcomp( census.subct[3:30], scale = TRUE )


pve <- (ct.pca$sdev^2)/(sum(ct.pca$sdev^2))

cum_pve <- cumsum(pve) 

par(mfrow=c(1, 2))

plot(pve, type="l", lwd=3, xlab="Principal Components", ylab="PVE", main="PVE (county)", cex=0.5)
plot(cum_pve, type="l", lwd=3, xlab="Principal Components", ylab="PVE", main="Cumulative PVE (county)")
abline(a=0.9,b=0)

pve2 <- (subct.pca$sdev^2)/(sum(subct.pca$sdev^2))

cum_pve2 <- cumsum(pve2) 

par(mfrow=c(1, 2))

plot(pve2, type="l", lwd=3, xlab="Principal Components", ylab="PVE", main="PVE (subcounty)")
plot(cum_pve2, type="l", lwd=3, xlab="Principal Components", ylab="PVE", main="Cumulative PVE (subcounty)")
abline(a=0.9,b=0)
```


Looking at our PVE graphs we find that we must include 13 principal components for ct.pc and 16 principal components for subct.pc in order to have 90% of variance explained. 




# 15. changed 29 to 28

Now we will preform hierarchical clustering on the dataset census.ct below we see a summary of the groupings for our hierarchical clustering with 10 clusters of census.ct

```{r, echo=FALSE}
set.seed(0)
scale.census.ct <- scale(census.ct[3:27]) 
dist <- dist(scale.census.ct, method = 'euclidean')

tree.census <- hclust(dist, method = 'complete')
```


```{r, echo=FALSE}
tree.census <- cutree(tree.census, k = 10)

kable(table(tree.census)) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                                              full_width=FALSE)
```


```{r, echo=FALSE}
set.seed(0)
scale.census.ct <- scale(ct.pca$x[,1:2]) 
dist <- dist(scale.census.ct, method = 'euclidean')

tree.pc <- hclust(dist, method = 'complete')
```


```{r, echo=FALSE}
tree.pc <- cutree(tree.pc, k = 10)

kable(table(tree.pc)) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
 full_width=FALSE)
```

```{r, echo=FALSE, eval=FALSE}
tree.census[which(census.ct$County == "San Mateo")]

tree.pc[which(census.ct$County == "San Mateo")]
```

For our hierarchical cluster from census.ct, we have San Mateo clustered into group 1 and for our hierarchical clustering from the our data frame of the first two principal components, we have San Mateo clustered into group 7. It makes more sense for San Mateo to be clustered into group 1 because it is clustered with counties that are near it. The hierarchical clustering with the first two principal components is off because the first two principal components dont encapsulate all of the variability in our data. 



```{r, echo=FALSE, eval=FALSE}
tmpwinner <- county_winner %>% ungroup %>%
  mutate(state = state.name[match(state, state.abb)]) %>%               ## state abbreviations
  mutate_at(vars(state, county), tolower) %>%                           ## to all lowercase
  mutate(county = gsub(" county| columbia| city| parish", "", county))  ## remove suffixes
tmpcensus <- census.ct %>% ungroup() %>% mutate_at(vars(State, County), tolower)

election.cl <- tmpwinner %>%
  left_join(tmpcensus, by = c("state"="State", "county"="County")) %>% 
  na.omit

## save meta information
election.meta <- election.cl %>% select(c(county, fips, state, votes, pct, total))

## save predictors and class labels
election.cl = election.cl %>% select(-c(county, fips, state, votes, pct, total))
```



```{r, echo=FALSE}
set.seed(10) 
n <- nrow(election.cl)
in.trn <- sample.int(n, 0.8*n) 
trn.cl <- election.cl[ in.trn,]
tst.cl <- election.cl[-in.trn,]
```


```{r, echo=FALSE}
set.seed(20) 
nfold <- 10
folds <- sample(cut(1:nrow(trn.cl), breaks=nfold, labels=FALSE))
```


```{r, echo=FALSE}
calc_error_rate = function(predicted.value, true.value){
  return(mean(true.value!=predicted.value))
}
records = matrix(NA, nrow=3, ncol=2)
colnames(records) = c("train.error","test.error")
rownames(records) = c("tree","logistic","lasso")
```


# 16. check errors

Now we will train a decision tree on our trn.cl dataset.

```{r, echo=FALSE, eval=FALSE}
set.seed(10)
tree <- tree(data = trn.cl, candidate ~ .) 


tree.cv <- cv.tree(tree, FUN = prune.misclass, K = nfold)
tree.cv
tree.cv <- prune.misclass(tree, best=6)
```

```{r, echo=FALSE}
draw.tree(tree, cex = 0.3)
```



From cross validation we find that we want to prune our tree to classify into 6 categories. 

```{r, echo=FALSE}
draw.tree(tree.cv, cex = 0.6)
```


From the tree above we can see some good trends in the countries voting behavior. We see that if a county with a smaller proportion of white people, and men tend to prefer Clinton over Trump. On the other hand, Trump preforms better in white counties that are around 50%+ men. Donald Trump also outpreforms Clinton in smaller population white counties, where a small percentage of citizens use public transit.





```{r, echo=FALSE}
trn.cl$candidate <- as.factor(ifelse(trn.cl$candidate == 'Donald Trump', 'Donald Trump', 'Hillary Clinton'))
tst.cl$candidate <- as.factor(ifelse(tst.cl$candidate == 'Donald Trump', 'Donald Trump', 'Hillary Clinton'))
#Training Error
treeTrainPred <- predict(tree.cv, trn.cl, type = 'class')
treeTrainError <- calc_error_rate(treeTrainPred, trn.cl$candidate)

#Test Error
treeTestPred <- predict(tree.cv, tst.cl, type = 'class')
treeTestError <- calc_error_rate(treeTestPred, tst.cl$candidate)
```


Lastly we will add the training and testing error of our pruned tree to our records matrix:

```{r, echo=FALSE}
records[1,] <- c(treeTrainError, treeTestError)
kable(records) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                                              full_width=FALSE)
```





# 17. Check errors


```{r, echo=FALSE, warning=FALSE}
logfit <- glm(candidate ~ ., data = trn.cl, family = 'binomial')
#summary(logfit)
```

Now we will rina logistic regression to predict the winning candidate in each county. From the summary of our logistic regression, we see that the predictors TotalPop, Men, White, Citizen, Income, IncomePerCap, Professional, Service, Production, Drive, Carpool, WorkAtHome, MeanCommute, Employed, PrivateWork, FamilyWork, and Unemployment are all significant. Comparing these predictors to our decision tree, we see similarities in all but transit. Transit is included in our tree model but not significant in our logistic regression. Looking at some of the more significant predictors in our logistic regression model we see that every percent increase in Unemployment corresponds to a 0.2011 increase in odd that the candidate Donald Trump wins. Similarly, every percent increase in proportion of citizens in the county corresponds to a 0.1377 increase in odds that Donald Trump wins the county. 


Lastly, we will add the training and test errors of our logistic fit to our records matrix:
```{r, echo=FALSE, warning=FALSE}
trn.cl$candidate <- ifelse(trn.cl$candidate=="Donald Trump", 0, 1)
tst.cl$candidate <- ifelse(tst.cl$candidate=="Donald Trump", 0,1)
#logTrainPred <- as.factor(ifelse(logTrainPred >= .5, 'Hillary Clinton', 'Donald Trump'))

log_train_pred <- predict(logfit, newdata = trn.cl)
log_test_pred <- predict(logfit, newdata = tst.cl)

trn.cl <- trn.cl %>% mutate(CAND = as.factor(ifelse(log_train_pred<=0.5,0,1)))
tst.cl <- tst.cl %>% mutate(CAND = as.factor(ifelse(log_test_pred<=0.5,0,1)))



logTrainError <- calc_error_rate(trn.cl$CAND, trn.cl$candidate)
logTestError <- calc_error_rate(tst.cl$CAND, tst.cl$candidate)


records[2,] <-c(logTrainError, logTestError)
 
kable(records) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                                               full_width=FALSE)
trn.cl <- subset(trn.cl, select = -CAND)
tst.cl <- subset(tst.cl, select = -CAND)
trn.cl$candidate <- as.factor(ifelse(trn.cl$candidate== 0, 'Donald Trump', 'Hillary Clinton'))
tst.cl$candidate <- as.factor(ifelse(tst.cl$candidate== 0, 'Donald Trump', 'Hillary Clinton'))
#summary(logfit)
```



# 18. Check errors

When creating our logistic fit we found that we have complete separation which is a sign that we may be overfitting our model. 

```{r, echo=FALSE}
set.seed(0)
x <- model.matrix(candidate ~ ., data = trn.cl)[,-1]
y <- as.factor(ifelse(trn.cl$candidate=='Hillary Clinton', 'Hillary Clinton', 'Donald Trump'))
lbda <- c(1, 5, 10, 50) * 1e-4

lasso.fit <- glmnet(x, y,  alpha = 1, lambda = lbda, family = 'binomial')

lasso.fit.cv <- cv.glmnet(x, y, alpha = 1, family = 'binomial', nfolds = nfold)

best <- lasso.fit.cv$lambda.min
lasso.fit <- glmnet(x, y, alpha = 1, family = 'binomial', lambda = best)
#predict(lasso.fit, type="coefficients", s=best)
```

Using LASSO regression, we find that the best lambda value to use is $\lambda = 0.00076031136$. We see that the non-zero coefficients using this lambda for a LASSO regression are the all the predictors except for ChildPverty, OtherTransp, SelfEmploymed, and minority, which are all zero. Comparing this to a non-penalized model we see that almost all of the zero coefficients for our penalized model were unsignificant predictors for our unpenalized model. 


Saving the training error and test errors to our records matrix:
```{r, echo=FALSE}
#Training Error
lassoTrainPred <- predict(lasso.fit, newx = x, type = 'class', s = best)
lassoTrainError <- calc_error_rate(lassoTrainPred, trn.cl$candidate)


#Test Error
newX <- model.matrix(candidate~., data = tst.cl)[,-1]
lassoTestPred <- predict(lasso.fit, newx = newX, type = 'class', s = best)
lassoTestError <- calc_error_rate(lassoTestPred, tst.cl$candidate)


#Saving to our records matrix
records[3,] <- c(lassoTrainError, lassoTestError)

kable(records)  %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                                              full_width=FALSE)
```


# 19. 

Here we are looking at the ROC curve of our decision tree, logistic regression, and LASSO regression:
```{r, echo=FALSE}
y2 <- as.factor(ifelse(tst.cl$candidate=='Hillary Clinton', 'Hillary Clinton', 'Donald Trump'))
treePred2 <- predict(tree.cv, tst.cl)
treePREDICTION <- prediction(treePred[,"Hillary Clinton"], y2)
tree_log <- performance(treePREDICTION, measure = "tpr", x.measure = "fpr")
```





```{r, echo=FALSE, warning=FALSE}
logTestPred <- predict(logfit, tst.cl, type = 'response')
logTestPREDICTION <- prediction(logTestPred, y2)
perf_log <- performance(logTestPREDICTION, measure = "tpr", x.measure = "fpr")
```



```{r, echo=FALSE}
lassoTestPred <- predict(lasso.fit, newx=model.matrix(candidate ~ ., data = tst.cl)[,-1], type = 'response', s=best)
lassoTestPREDICTION <- prediction(lassoTestPred, y2)
perf_lasso <- performance(lassoTestPREDICTION, measure = "tpr", x.measure = "fpr")
plot(perf_log, col=2, lwd=1, main="ROC curves")
plot(tree_log, col=3, lwd=1,  add = TRUE)
plot(perf_lasso, col=4, lwd=1, add = TRUE)
abline(0, 1)
```


Looking at the results of our ROC curves we see that the red curve corresponds to logistic regression, the green curve corresponds to our decision tree, and the blue curve corresponds to our LASSO regression. We see that our logistic regression model has the best misclassification error; however, due to the fact that we got 0 or 1 for some of our predicted probabilites, this model may be overfit to the data. to account for this we ran a LASSO regression which out performed our decision tree model. This may be becuase our final pruned decision tree only took into account 4 predictors out of a total of 27 total predictors. Thus, due to the high amount of possible predictors, the decision tree may have left out significant predictors that were taken into account for our logistic and LASSO regression models. 



# 20.

```{r, echo=FALSE}
lda.fit <- lda(trn.cl$candidate~., data=trn.cl)


lda_train_pred <- predict(lda.fit, newdata=trn.cl)
lda_test_pred <- predict(lda.fit, newdata=tst.cl)

lda_train_error <- calc_error_rate(lda_train_pred[[1]], trn.cl$candidate)
lda_test_error <- calc_error_rate(lda_test_pred[[1]], tst.cl$candidate)

add <- matrix(NA, nrow = 1, ncol = 2)
rownames(add) <- 'LDA'
records <- rbind(records, add)
records[4,] <- c(lda_train_error, lda_test_error)

kable(records)  %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                                              full_width=FALSE)
```




Here we will take a look at the ROC curve for LDA:


```{r, echo=FALSE}
lda_PREDICTION <- prediction(lda_test_pred[[3]], y2)
lda_perf <- performance(lda_PREDICTION, measure = "tpr", x.measure = "fpr")
plot(lda_perf, col=6, lwd=1, main="ROC curve for LDA" )
abline(0,1)
```



Now adding this curve to our ROC curves graph above:



```{r, echo=FALSE}
plot(perf_log, col=2, lwd=1, main="ROC curves")
plot(tree_log, col=3, lwd=1,  add = TRUE)
plot(perf_lasso, col=4, lwd=1, add = TRUE)
plot(lda_perf, col=6, lwd=1, add = TRUE)
abline(0, 1)
```
















```{r, include = FALSE}
save.image('FinalProject.RData')
```